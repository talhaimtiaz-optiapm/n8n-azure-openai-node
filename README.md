# n8n Azure OpenAI Node

A high-performance, production-ready Azure OpenAI node for n8n with optimized batch processing and user-friendly interface.

## üöÄ Features

- **High Performance**: 60-70% faster than basic implementations with parallel batch processing
- **User-Friendly**: Smart dropdowns for deployment and concurrency selection
- **Production Ready**: Rate limit aware with configurable concurrent requests
- **Azure OpenAI Compatible**: Full support for Azure OpenAI chat completions
- **Error Handling**: Comprehensive error handling with detailed feedback
- **Output Format**: Compatible with native OpenAI node format

## üìä Performance

- **Original Implementation**: 13 minutes for large batches
- **This Node**: 4-6 minutes for the same workload
- **Improvement**: ~65-70% faster execution time

## üõ†Ô∏è Installation

### Prerequisites

- n8n version 1.0+ (required for routing system)
- Node.js 18+ 
- Azure OpenAI deployment with API access

### Install the Node

1. **Clone this repository:**
   ```bash
   git clone https://github.com/your-username/n8n-azure-openai-node.git
   cd n8n-azure-openai-node
   ```

2. **Install dependencies:**
   ```bash
   npm install
   ```

3. **Build the node:**
   ```bash
   npm run build
   ```

4. **Link to your n8n instance:**
   ```bash
   # For local n8n installations
   npm link
   cd /path/to/your/n8n
   npm link n8n-azure-openai-node

   # Or copy to n8n custom nodes directory
   cp -r dist/* ~/.n8n/custom/
   ```

## üîß Configuration

### Azure OpenAI Credentials

Create Azure OpenAI credentials in n8n:

```
Credential Type: Azure OpenAI API
- API Key: Your Azure OpenAI API key
- Endpoint: https://your-resource.openai.azure.com (no trailing slash)
- API Version: 2025-01-01-preview (or your preferred version)
- Deployment: Your default deployment name (e.g., gpt-4o-mini)
```

### Node Configuration

#### Deployment Selection
- **Use Credential Default**: Uses deployment from credentials
- **GPT-4o Mini**: Uses "gpt-4o-mini" deployment
- **GPT-4o**: Uses "gpt-4o" deployment  
- **Custom**: Enter your specific deployment name

#### Concurrent Requests
- **Single (1)**: One request at a time (safest, default)
- **Low (2)**: Conservative parallel processing
- **Medium (6)**: Balanced performance and safety
- **High (10)**: Fast processing (check rate limits)
- **Custom (1-20)**: Specify exact concurrent requests

## üéØ Usage Examples

### Basic Chat Completion

```json
{
  "deployment": "Use Credential Default",
  "concurrent_requests": "Single (1)",
  "messages": [
    {
      "role": "user", 
      "content": "Hello, how are you?"
    }
  ]
}
```

### High-Performance Batch Processing

For processing many items efficiently:

1. **Use Loop Node** with batch size matching concurrent requests:
   - Loop Batch Size: 6
   - Azure Node Concurrent: "Medium (6)"

2. **For 192 items example**:
   - 192 √∑ 6 = 32 iterations
   - ~2 seconds per iteration = ~64 seconds total

## üìà Performance Optimization

### Batch Processing Setup

**Optimal Configuration:**
```
Loop Node Batch Size = Azure Node Concurrent Requests
```

**Example Configurations:**

| Items | Loop Batch | Azure Concurrent | Iterations | Time |
|-------|------------|------------------|------------|------|
| 50 | 5 | Single (1) | 50 | ~100s |
| 50 | 10 | High (10) | 5 | ~10s |
| 200 | 6 | Medium (6) | 34 | ~68s |
| 200 | 10 | High (10) | 20 | ~40s |

### Rate Limit Guidelines

**Conservative (Production)**:
- Single (1) or Low (2) concurrent requests
- Guaranteed to work within Azure limits

**Balanced (Development)**:  
- Medium (6) concurrent requests
- Good balance of speed and safety

**Aggressive (Testing)**:
- High (10) or Custom (15-20)
- Maximum speed, monitor for rate limits

## üîß Advanced Configuration

### Additional Fields

The node supports all Azure OpenAI parameters:

- **Max Tokens**: Maximum tokens to generate (default: 1000)
- **Temperature**: Randomness control (0-2, default: 0.7)
- **Top P**: Nucleus sampling (0-1, default: 1.0)
- **Frequency Penalty**: Reduce repetition (-2 to 2)
- **Presence Penalty**: Encourage new topics (-2 to 2)
- **Stop Sequences**: Comma-separated stop sequences
- **User ID**: Unique identifier for tracking

### Output Format

**Simplified (Default)**: Returns clean choice object
```json
{
  "index": 0,
  "message": {
    "role": "assistant",
    "content": "Hello! How can I help you?",
    "refusal": null,
    "annotations": []
  },
  "logprobs": null,
  "finish_reason": "stop"
}
```

**Full Response**: Complete Azure OpenAI response with usage stats, etc.

## üö® Troubleshooting

### Common Issues

**429 Rate Limit Errors**:
- Reduce concurrent requests to Single (1) or Low (2)
- Check your Azure OpenAI deployment quotas
- Ensure Loop batch size matches concurrent requests

**404 Resource Not Found**:
- Verify deployment name is correct
- Check endpoint URL (no trailing slash)
- Confirm API version compatibility

**Timeout Errors**:
- Azure OpenAI may be experiencing high load
- Try reducing concurrent requests
- Check network connectivity

**Memory Issues**:
- Reduce Loop batch size for very large datasets
- Monitor n8n memory usage during execution

### Performance Issues

**Slow Execution**:
- Ensure Loop batch size matches Azure concurrent requests
- Check if you're hitting rate limits (reduces to sequential processing)
- Verify network latency to Azure region

**Inconsistent Results**:
- Set temperature to 0 for deterministic outputs
- Use the same deployment consistently
- Check for content filtering (Azure-specific)

## üîç Monitoring

### Performance Metrics

Monitor these metrics for optimal performance:
- **Requests per minute**: Should match your concurrent setting
- **Average response time**: 1-3 seconds for most deployments
- **Error rate**: Should be < 1% in production
- **Token usage**: Track for cost management

## ü§ù Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

### Development

```bash
# Install dependencies
npm install

# Build in watch mode
npm run dev

# Run linting
npm run lint

# Fix linting issues
npm run lintfix
```

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE.md](LICENSE.md) file for details.

## üôè Acknowledgments

- Built for the n8n community
- Optimized based on real-world performance analysis
- Inspired by native n8n node architecture

## üìû Support

- **Issues**: [GitHub Issues](https://github.com/your-username/n8n-azure-openai-node/issues)
- **Documentation**: This README and inline code comments
- **Community**: n8n Community Forum

---

**Made with ‚ù§Ô∏è for the n8n community**
